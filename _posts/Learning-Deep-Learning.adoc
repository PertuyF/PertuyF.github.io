= Learning DeepLearning

// See https://hubpress.gitbooks.io/hubpress-knowledgebase/content/ for information about the parameters.
:hp-image: /images/covers/brain.png
:published_at: 2017-05-26
:hp-tags: DeepLearning, Python, MOOC, Blog
// :hp-alt-title: My English Title

[abstract]
--
The role of intuition
--

=== The role of Intuition

I started reading and experimenting about *Deep Learning* not so long ago and I found a great discrepancy in the affordability of resources available to _naïve_ learners.
I've met a similar situation in image processing, where explanations about how an algorithm works and what it does is too often limited to inapprehensible equations.

I must admit, I am not of the _math-oriented_ type.
Well, at least I am not fond of math in the way it is taught by most mathematicians.
I need intuition to understand how a tool works and how I can use it.
And a good graphical representation is most of the time far more efficient for me than any equation.
For me, learning is all about building my own representation of things, that fits with what I already know.

Anyway, there are other people that feel the same. I know that because intuitive approaches tends to increase significantly over the web.
https://betterexplained.com/[Better Explained] is a good example, I would have dreamt to be taught math this way in school!

In short, I claim that getting intuition is how it works.
Intuition is *_FUNDAMENTAL_* to acquire knowledge.

=== Grabbing intuition about Deep Learning

Hopefully, some people decided to created resources about Deep Learning that follow this philosophy.
These resources are meant to bring Deep Learning to anyone with minimal skills in informatics and enough curiosity.
They aim to help building intuition out of practical examples, and allow anyone to learn in a self-paced schedule.

There are two resources that exactly fit this description, and that I would like to share because of the outstanding quality of their content:

 * https://cloud.google.com/blog/big-data/2017/01/learn-tensorflow-and-deep-learning-without-a-phd[DeepLearning without a PhD], a 3 hours speech by https://plus.google.com/+MartinGorner[Martin Görner] (Google) that demonstrate how to build a Neural Network working with the famous MNIST dataset.
 Several architectures are tested, demonstrating how to tune the network up to a *Convolutional Neural Network (CNN)* that achieves up to 99.5% accuracy (top 10 of best results for this dataset).
 Final part is a demo of some *Recurrent Neural Network (RNN)* able to generate random text following the style of Shakespeare's play, or the syntax of the Python code of Tensorflow itself.
 Both the videos (short and long versions), the code, the slides and an interactive codelab of this excellent demonstration are available.

 * http://course.fast.ai/index.html[Practical Deep Learning For Coders], a 7 weeks formation by https://www.usfca.edu/data-institute/about-us/researchers[Jeremy Howard] (fast.ai, Data Institute at the University of San Francisco), freely available as a MOOC.
 The whole course is covering *CNN*, demonstrating the use of pre-trained models, *RNN* and *Natural Language Processing(NLP)*.
 Each week corresponds to a ~2H video, plus a Jupyter notebook and assignments.
 Usually, students are encouraged to enter Kaggle competitions and play around with all resources available.
 The website also has a nice and responsive forum with a growing community.
 Part 2 of this course should be released in the next months, and I am sure it will be as amazing as Part 1.

The common aim of these two resources is to get to practice as soon as possible.
Only then dissect who does what, why and how.
The fastest way to build intuition is to observe the thing running (or literally stare at it), play with it and think about it.

=== Getting practical: get a computer

But, playing with *Neural Networks* is not exactly trivial, not if you don't want to get bored very quickly. +
Why that? +
Because they're hungry for computational power.

Well, they are not exactly monsters that will require thousands to be invested in your computer, they can run on pretty any system available these days.
But they can be so *slooowwwwww...*

The point is that Neural networks rely on matrix computations, with thousands of computations in parallel.
So depending on your hardware, you will be able to train a network in a few seconds.
Or to wait for *hours* (Seriously. Hours.) for the result.
And that's why it is not trivial: to keep it playful you have to get some capable hardware, then set up your software properly before everything works.
Basically there are two options:

 * *Rent a dedicated system*, such as computation servers (which are getting cheaper and cheaper).
  It seems to work quite well, but not everyone is willing to invest money in it.
  I am not, so I will not detail this option any further, but it's still a viable way to do things.
 * *Get your own system*.
 Hopefully, you don't need an extreme CPU for that, because matrix computations are totally adapted to GPUs.
 And GPUs are affordable and very well spread (Thank you Gamers!).
 Almost any mid-range GPU will be able to get you started if needed, but there are definitely important criterions to be taken into account if you want to get serious job done.
  http://timdettmers.com/2017/04/09/which-gpu-for-deep-learning/[Which GPU to get for Deep Learning] is very nice article on this subject by Tim Dettmers. The author is very responsive, so do not hesitate to ask for more precisions in the comments.

=== Getting practical: make it run!

Once you get a suitable system, it's time to choose the software and to get it running.
There is certainly no _best_ language for Deep Learning, and I will not pretend to recommend any.
On my side I use *Python*, and obviously the resources I am talking about are using Python too.

Many Python libraries are available for Deep Learning, and the choice might be difficult here also.
I think https://keras.io/[Keras] is a good pick, as it is unifying some popular frameworks so you can jump from one to another very easily.
https://www.tensorflow.org/[TensorFlow] and http://deeplearning.net/software/theano/#[Theano] are fully supported, and https://docs.microsoft.com/en-us/cognitive-toolkit/[CNTK] and http://mxnet.io/[MXNET] are coming right now (beta).
Reviews of the different frameworks are available online (for example https://indico.io/blog/python-deep-learning-frameworks-reviewed/[here]).

There are plenty tutorial and use cases available online that use Keras to help at start.

=== The next step

The great thing about Deep Learning is that there is so much to understand, to explore about it.
So that any tiny idea could lead to discover useful notions.
These times will probably be referred to as the golden age of Deep Learning a few years ahead, and the most people getting involved, the more progress to be done, so let's get involved!

Basically I am not very far in the process.
I integrated basic notions about neural networks: basic architectures, main parameters to tune, how to detect common problems and how to improve the results.

I feel that I know just enough to understand I know nothing (you're not alone, John Snow).
I like to think about *Deep Learning* as a *Black Hole*: everyone understand it is extremely powerful, some people have an idea of how to use it, but no one knows for sure what's going on inside.

On my side I'll try to keep a methodical, scientific, approach to investigate this field, and communicate about it to benefit from peer review.
I do not plan to look at scientific literature before testing an idea, but in a second time just to ensure I am not re-discovering the wheel.
What I expect is to get insights from other people and to exchange ideas, so tell me if you think differently, or if you experimented similar things.

All I want is to learn from our discussions.
